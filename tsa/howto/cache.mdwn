A caching service is a set of services keeping a smaller cache of
content in memory to speed up access to resources on a slower backend
server.

[[!toc levels=3]]

# Design

## Nginx

picking the "light" debian package. The modules that would be
interesting in others would be "cache purge" (from extras) and "geoip"
(from full):

    apt install nginx-light

## ATS

    apt install trafficserver

Default Debian config seems sane when compared to the [Cicimov
tutorial][cicimov]. On thing we will need to change is the [default listening
port][], which is by default:

[default listening port]: https://docs.trafficserver.apache.org/en/8.0.x/admin-guide/files/records.config.en.html#proxy.config.http.server_ports

    CONFIG proxy.config.http.server_ports STRING 8080 8080:ipv6

We want something more like this:

    CONFIG proxy.config.http.server_ports STRING 80 80:ipv6 443:ssl 443:ssl:ipv6

Then we also need to configure the path to the SSL certs, we use the
self-signed certs for benchmarking:

    CONFIG proxy.config.ssl.server.cert.path STRING /etc/ssl/torproject-auto/servercerts/
    CONFIG proxy.config.ssl.server.private_key.path STRING /etc/ssl/torproject-auto/serverkeys/

We need to add trafficserver to the `ssl-cert` group so it can read
those:

    adduser trafficserver ssl-cert

Then we setup this remapping rule:

    map https://blog.torproject.org/ https://backend.example.com/

(`backend.example.com` is the prod alias of our backend.)

And finally curl is able to talk to the proxy:

    curl --proxy-cacert /etc/ssl/torproject-auto/servercerts/ca.crt --proxy https://cache01.torproject.org/ https://blog.torproject.org

TODO: proxy fails to hit backend:

    curl: (56) Received HTTP code 404 from proxy after CONNECT

Same with plain `GET`:

    # curl -s -k -I --resolve *:443:127.0.0.1 https://blog.torproject.org | head -1
    HTTP/1.1 404 Not Found on Accelerator

It seems that the backend needs to respond on the right-side of the
remap rule correctly, as ATS doesn't reuse the `Host` header
correctly, which is kind of a problem because the backend wants to
redirect everything to the canonical hostname for SEO purposes. We
*could* tweak that and make `backend.example.com` the canonical host,
but then it would make disaster recovery much harder, and could make
some links point there instead of the real canonical host.

I tried the mysterious regex_remap plugin:

    map http://cache01.torproject.org/ http://localhost:8000/ @plugin=regex_remap.so @pparam=maps.reg @pparam=host

with this in `maps.reg`:

    .* $s://$f/$P/

... which basically means "redirect everything to the original scheme,
host and path", but that (obviously, maybe) fails with:

    # curl -I -s http://cache01.torproject.org/ | head -1
    HTTP/1.1 400 Multi-Hop Cycle Detected

It feels it *really* doesn't want to act as a transparent proxy...

I also tried a header rewrite:

    map http://cache01.torproject.org/ http://localhost:8000/ @plugin=header_rewrite.so @pparam=rules1.conf

with `rules1.conf` like:

    set-header host cache01.torproject.org
    set-header foo bar

... and the `Host` header is untouched. The rule works though because
the `Foo` header appears in the request.

Solution:

    CONFIG proxy.config.url_remap.pristine_host_hdr INT 1

It's clearly stated in [the tutorial](https://docs.trafficserver.apache.org/en/latest/admin-guide/configuration/redirecting-http-requests.en.html), but mistakenly in
[Cicimov's][cicimov].

Next hurdle: no HTTP/2 support, even when using `proto=http2;http`
(falls back on `HTTP/1.1`) and `proto=http2` only (fails with
`WARNING: Unregistered protocol type 0`).

### Preliminary benchmarks

With `blog.tpo` in `/etc/hosts`, because `proxy-host` doesn't work, and
running on the same host as the proxy (!), cold cache:

    root@cache01:~# siege https://blog.torproject.org/
    ** SIEGE 4.0.4
    ** Preparing 100 concurrent users for battle.
    The server is now under siege...
    Lifting the server siege...
    Transactions:                  68068 hits
    Availability:                 100.00 %
    Elapsed time:                 119.53 secs
    Data transferred:             654.47 MB
    Response time:                  0.18 secs
    Transaction rate:             569.46 trans/sec
    Throughput:                     5.48 MB/sec
    Concurrency:                   99.67
    Successful transactions:       68068
    Failed transactions:               0
    Longest transaction:            0.56
    Shortest transaction:           0.00

Warm cache:

    root@cache01:~# siege https://blog.torproject.org/
    ** SIEGE 4.0.4
    ** Preparing 100 concurrent users for battle.
    The server is now under siege...
    Lifting the server siege...
    Transactions:                  65953 hits
    Availability:                 100.00 %
    Elapsed time:                 119.71 secs
    Data transferred:             634.13 MB
    Response time:                  0.18 secs
    Transaction rate:             550.94 trans/sec
    Throughput:                     5.30 MB/sec
    Concurrency:                   99.72
    Successful transactions:       65953
    Failed transactions:               0
    Longest transaction:            0.62
    Shortest transaction:           0.00

And `traffic_top` looks like this after the second run:

             CACHE INFORMATION                     CLIENT REQUEST & RESPONSE        
    Disk Used   77.8K    Ram Hit     99.9%   GET         98.7%    200         98.3%
    Disk Total 268.1M    Fresh       98.2%   HEAD         0.0%    206          0.0%
    Ram Used    16.5K    Revalidate   0.0%   POST         0.0%    301          0.0%
    Ram Total  352.3K    Cold         0.0%   2xx         98.3%    302          0.0%
    Lookups    134.2K    Changed      0.1%   3xx          0.0%    304          0.0%
    Writes      13.0     Not Cache    0.0%   4xx          2.0%    404          0.4%
    Updates      1.0     No Cache     0.0%   5xx          0.0%    502          0.0%
    Deletes      0.0     Fresh (ms)   8.6M   Conn Fail    0.0     100 B        0.1%
    Read Activ   0.0     Reval (ms)   0.0    Other Err    2.8K    1 KB         2.0%
    Writes Act   0.0     Cold (ms)   26.2G   Abort      111.0     3 KB         0.0%
    Update Act   0.0     Chang (ms)  11.0G                        5 KB         0.0%
    Entries      2.0     Not (ms)     0.0                         10 KB       98.2%
    Avg Size    38.9K    No (ms)      0.0                         1 MB         0.0%
    DNS Lookup 156.0     DNS Hit     89.7%                        > 1 MB       0.0%
    DNS Hits   140.0     DNS Entry    2.0   
                 CLIENT                                ORIGIN SERVER                
    Requests   136.5K    Head Bytes 151.6M   Requests   152.0     Head Bytes 156.5K
    Req/Conn     1.0     Body Bytes   1.4G   Req/Conn     1.1     Body Bytes   1.1M
    New Conn   137.0K    Avg Size    11.0K   New Conn   144.0     Avg Size     8.0K
    Curr Conn    0.0     Net (bits)  12.0G   Curr Conn    0.0     Net (bits)   9.8M
    Active Con   0.0     Resp (ms)    1.2   
    Dynamic KA   0.0                        
    cache01                                    (r)esponse (q)uit (h)elp (A)bsolute

ab:

    Server Software:        ATS/8.0.2
    Server Hostname:        blog.torproject.org
    Server Port:            443
    SSL/TLS Protocol:       TLSv1.2,ECDHE-RSA-AES256-GCM-SHA384,2048,256
    Server Temp Key:        X25519 253 bits
    TLS Server Name:        blog.torproject.org

    Document Path:          /
    Document Length:        52873 bytes

    Concurrency Level:      100
    Time taken for tests:   1.248 seconds
    Complete requests:      1000
    Failed requests:        0
    Total transferred:      53974000 bytes
    HTML transferred:       52873000 bytes
    Requests per second:    801.43 [#/sec] (mean)
    Time per request:       124.776 [ms] (mean)
    Time per request:       1.248 [ms] (mean, across all concurrent requests)
    Transfer rate:          42242.72 [Kbytes/sec] received

    Connection Times (ms)
                  min  mean[+/-sd] median   max
    Connect:        8   47  20.5     46     121
    Processing:     6   75  16.2     76     116
    Waiting:        1   13   6.8     12      49
    Total:         37  122  21.6    122     196

    Percentage of the requests served within a certain time (ms)
      50%    122
      66%    128
      75%    133
      80%    137
      90%    151
      95%    160
      98%    169
      99%    172
     100%    196 (longest request)


# Discussion

A discussion of the design of the new service, mostly.

## Overview

The original goal of this project is to create a pair of caching
servers in front of the blog to reduce the bandwidth costs we're being
charged there.

## Goals

### Must have

 * reduce the traffic on the blog, hosted at a costly provider (#32090)
 * HTTPS support in the frontend and backend
 * deployment through Puppet
 * anonymized logs

### Nice to have

 * provide a frontend for our existing mirror infrastructure, a
   home-made CDN for TBB and other releases
 * no on-disk logs
 * cute dashboard or grafana integration
 * well-maintained upstream Puppet module

### Approvals required

 * approved and requested by vegas

## Non-Goals

 * global CDN for users outside of TPO
 * geoDNS

## Proposed Solution

TBD (To Be Determined). Considering either Apache Traffic Server or
Nginx on a pair of servers.

TODO: In the gnt-fsn cluster if cost-effective, otherwise maybe in two
Hetzner VMs?

## Launch checklist

See [#32239](https://trac.torproject.org/projects/tor/ticket/32239).

## Benchmarking procedures

Will require a test VM (or two?) to hit the caches.

### Common procedure

 1. punch a hole in the firewall to allow cache2 to access cache1

        iptables -I INPUT -s 78.47.61.104 -j ACCEPT
        ip6tables -I INPUT -s 2a01:4f8:c010:25ff::1 -j ACCEPT

 2. point the blog to cache1 on cache2:

        116.202.120.172	blog.torproject.org
        2a01:4f8:fff0:4f:266:37ff:fe26:d6e1 blog.torproject.org

 3. launch the benchmark

### Siege

Siege configuration sample:

```
verbose = false
fullurl = true
concurrent = 100
time = 2M
url = http://www.example.com/
delay = 1
internet = false
benchmark = true
```

Might require this, which might work only with varnish:

```
proxy-host = 209.44.112.101
proxy-port = 80
```

Alternative is to hack `/etc/hosts`.

### apachebench

Classic commandline:

    ab2 -n 1000 -c 100 -X cache01.torproject.org https://example.com/

`-X` also doesn't work with ATS, hacked `/etc/hosts`.

### Other tools

Siege has trouble going above ~100 concurrent clients because of its
design (and ulimit) limitations. Its interactive features are also
limited, here's a set of interesting alternatives:

 * [bombardier](https://github.com/codesenberg/bombardier) - golang, HTTP/2, better performance than siege in
   my (2017) tests
 * [boom](https://github.com/tarekziade/boom) - python rewrite of apachebench, supports duration,
   HTTP/2, not in debian, unsearchable name
 * [go-wrk](https://github.com/adjust/go-wrk/) - golang rewrite of wrk with HTTPS, had performance
   issues in my first tests (2017), [no duration target](https://github.com/adjust/go-wrk/issues/2), not in
   Debian
 * [hey](https://github.com/rakyll/hey) - golang rewrite of apachebench, similar to boom, not in
   debian, unsearchable name
 * [Jmeter](https://jmeter.apache.org/) - interactive behavior, can replay recorded sessions
   from browsers
 * [Locust](https://locust.io/) - distributed, can model login and interactive
   behavior, not in Debian
 * [Tsung](http://tsung.erlang-projects.org/1/01/about/) - multi-protocol, distributed, erlang
 * [wrk](https://github.com/wg/wrk/) - multithreaded, epoll, Lua scriptable, no HTTPS

## Cost

Somewhere between 11EUR and 100EUR/mth for bandwidth and hardware.

We're getting apparently around 2.2M "page views" per month at
Pantheon. That is about 1 hit per second and 12 terabyte per month,
36Mbit/s on average:

    $ qalc
    > 2 200 000 ∕ (30d) to hertz

      2200000 / (30 * day) = approx. 0.84876543 Hz

    > 2 200 000 * 5Mibyte

      2200000 * (5 * mebibyte) = 11.534336 terabytes

    > 2 200 000 * 5Mibyte/(30d) to megabit / s

      (2200000 * (5 * mebibyte)) / (30 * day) = approx. 35.599802 megabits / s

Hetzner charges 1EUR/TB/month over our 1TB quota, so bandwidth would
cost 11EUR/month on average. If costs become prohibitive, we could
switch to a Hetzner VM which includ 20TB of traffic per month at costs
ranging from 3EUR/mth to 30EUR/mth depending on the VPS size (between
1 vCPU, 2GB ram, 20GB SSD and 8vCPU, 32GB ram and 240GB SSD).

Dedicated servers start at 34EUR/mth (`EX42`, 64GB ram 2x4TB HDD) for
unlimited gigabit.

## Alternatives considered

Four alternatives were seriously considered:

 * Apache Traffic Server
 * Nginx proxying + caching
 * Varnish + stunnel
 * Fastly

Other alternatives were not:

 * [Apache HTTPD caching](https://httpd.apache.org/docs/2.4/caching.html) - performance expected to be sub-par
 * [Envoy][] - [not designed for caching](https://github.com/envoyproxy/envoy/issues/868), [external cache support
   planned in 2019](https://blog.getambassador.io/envoy-proxy-in-2019-security-caching-wasm-http-3-and-more-e5ba82da0197?gi=82c1a78157b8)
 * [HAproxy](https://www.haproxy.com/) - [not designed to cache large objects](https://www.haproxy.com/documentation/aloha/9-5/traffic-management/lb-layer7/caching-small-objects/)
 * [Ledge](https://github.com/ledgetech/ledge) - caching extension to Nginx with ESI, Redis, and cache
   purge support, not packaged in Debian
 * [Nuster](https://github.com/jiangwenyuan/nuster) - new project, not packaged in Debian (based on
   HAproxy), performance [comparable with nginx and varnish](https://github.com/jiangwenyuan/nuster/wiki/Web-cache-server-performance-benchmark:-nuster-vs-nginx-vs-varnish-vs-squid#results)
   according to upstream, although impressive improvements
 * [Polipo](https://en.wikipedia.org/wiki/Polipo) - not designed for production use
 * [Squid](http://www.squid-cache.org/) - not designed as a reverse proxy
 * [Traefik](https://traefik.io/) - [not designed for caching](https://github.com/containous/traefik/issues/878)

[Envoy]: https://www.envoyproxy.io/

### Apache Traffic Server

#### Summary of online reviews

Pros:

 * HTTPS
 * HTTP/2
 * industry leader (behind cloudflare)
 * out of the box clustering support

Cons:

 * load balancing is an experimental plugin (at least in 2016)
 * no static file serving? or slower?
 * no commercial support

Used by Yahoo, Apple and Comcast.

#### First impressions

Cons:

 * configuration spread out over many different configuration file
 * complex and arcane configuration language (e.g. try to guess what
   this actually does:: `CONFIG proxy.config.http.server_ports STRING
   8080:ipv6:tr-full 443:ssl
   ip-in=192.168.17.1:80:ip-out=[fc01:10:10:1::1]:ip-out=10.10.10.1`)
 * configuration syntax varies across config files and plugins
 * <del>couldn't decouple backend hostname and passed `Host`
   header</del> bad random tutorial found on the internet
 * couldn't figure out how to make HTTP/2 work

Pros:

 * no query logging by default (good?)
 * good documentation, but a bit lacking in tutorials
 * nice little dashboard shipped by default (`traffic_top`) although
   it could be more useful (doesn't seem to show hit ratio clearly)

### Nginx

Pros:

 * provides full webserver stack means much more flexibility,
   possibility of converging over a single solution across the
   infrastructure
 * very popular
 * load balancing (but no active check in free version)
 * can serve static content
 * HTTP/2
 * HTTPS

Cons:

 * provides full webserver stack (!) means larger attack surface
 * no ESI or ICP?
 * does not cache out of the box, requires config which might imply
   lesser performance
 * opencore model with paid features, especially "active health
   checks", "Cache Purging API" (although there are [hackish ways to
   clear the cache](https://stackoverflow.com/questions/6236078/how-to-clear-the-cache-of-nginx) and [a module](https://github.com/nginx-modules/ngx_cache_purge)), and "session persistence
   based on cookies"
 * most plugins are statically compiled in different "flavors",
   although it's possible to have dynamic modules

Used by Cloudflare, [Dropbox](https://blogs.dropbox.com/tech/2017/09/optimizing-web-servers-for-high-throughput-and-low-latency/), MaxCDN and Netflix.

### Varnish

Pros:

 * specifically built for caching
 * very flexible
 * grace mode can keep objects even after TTL expired (when backends
   go down)
 * third most popular, after Cloudflare and ATS

Cons:

 * no HTTPS support on frontend or backend in the free version, would
   require stunnel hacks
 * configuration is compiled and a bit weird
 * static content needs to be generated in the config file, or sidecar
 * no HTTP/2 support

Used by Fastly.

### Fastly itself

We could just put Fastly in front of all this and shove the costs on
there.

Pros:

 * easy
 * possibly free

Cons:

 * might go over our quotas during large campaigns
 * sending more of our visitors to Fastly, non-anonymously

## Sources

Benchmarks:

 * [Bizety: Nginx vs Varnish vs Apache Traffic Server - High Level
   Comparison](https://www.bizety.com/2016/01/07/nginx-vs-varnish-vs-apache-traffic-server-high-level-comparison/) - "Each proxy server has strengths and weakness"
 * [ScaleScale: Nginx vs Varnish: which one is better?](https://www.scalescale.com/tips/nginx/nginx-vs-varnish/) - nginx +
   tmpfs good alternative to varnish
 * [garron.me: Nginx + Varnish compared to Nginx](https://www.garron.me/en/go2linux/nginx-varnish-vs-nginx-alone-compared.html) - equivalent
 * [Uptime Made Easy: Nginx or Varnish Which is Faster?](http://www.uptimemadeeasy.com/cloud/nginx-or-varnish-which-is-faster/) -
   equivalent
 * [kpayne.me: Apache Traffic Server as a Reverse Proxy](https://kpayne.me/2012/04/10/apache-traffic-server-as-a-reverse-proxy/) -
   "According to blitz.io, Varnish and Traffic Server benchmark
   results are close. According to ab, Traffic Server is twice as fast
   as Varnish"
 * [University of Oslo: Performance Evaluation of the Apache Traffic
   Server and Varnish Reverse Proxies](https://pdfs.semanticscholar.org/157b/bec8591a9fdb21e90831309f10ff6705b70d.pdf) - "Varnish seems the more
   promising reverse proxy server"
 * [Loggly: Benchmarking 5 Popular Load Balancers: Nginx, HAProxy,
   Envoy, Traefik, and ALB](https://www.loggly.com/blog/benchmarking-5-popular-load-balancers-nginx-haproxy-envoy-traefik-and-alb/)
 * [SpinupWP: Page Caching: Varnish Vs Nginx FastCGI Cache 2018
   Update](https://spinupwp.com/page-caching-varnish-vs-nginx-fastcgi-cache-2018/) - "Nginx FastCGI Cache is the clear winner when it comes
   to outright performance. It’s not only able to handle more requests
   per second, but also serve each request 55ms quicker on average."

Tutorials and documentation:

 * [Apache.org: Why Apache Traffic Server](https://svn.apache.org/repos/infra/websites/production/trafficserver/content/why-ats.html) - upstream docs
 * [czerasz.com: Nginx Caching Tutorial - You Can Run Faster](https://czerasz.com/2015/03/30/nginx-caching-tutorial/) -
   tutorial
 * [Igor Cicimov: Apache Traffic Server as Caching Reverse Proxy][cicimov] -
   tutorial, "Apache TS presents a stable, fast and scalable caching
   proxy platform"
 * [Datanyze.com: Web Accelerators Market Share Report](https://www.datanyze.com/market-share/accelerators)

[cicimov]: https://icicimov.github.io/blog/server/Apache-Traffic-Server-as-Caching-Reverse-Proxy/
